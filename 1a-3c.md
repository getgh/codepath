CSC 4520/6520
Design & Analysis of Algorithms
CHAPTER 1: INTRODUCTION
AB DULLAH BAL , PHD
1
Objectives
Data Structures Linear Data
Structures
Graphs Trees
2
Data Structures
o Since the vast majority of algorithms of interest operate on data, particular
ways of organizing data play a critical role in the design and analysis of
algorithms.
o A data structure can be defined as a particular scheme of organizing related
data items.
3
Data Structures
o Computers may be fast,
o But they are not infinitely fast.
o Computing time is therefore a bounded resource, which makes it precious.
o Memory may be inexpensive,
o But it is neither infinite nor free.
o That‚Äôs why, we should choose algorithms that use the resources of time and space
efficiently.
4
Data Structures
o A data structure is a way to store and organize data in order to facilitate access and
modifications.
o Using the appropriate data structure or structures is an important part of algorithm
design.
o No single data structure works well for all purposes, and so we should know the
strengths and limitations of several of them.
5
Fundamental Data Structures
o List
o Array
oLinked List
oString
o Stack
o Queue
o Priority Queue
o Graph
o Tree
o Set and Dictionary
6
Linear Data Structures: Array - Linked List
7
oThe two most important elementary data structures are the array and the linked
list.
```
oA (one-dimensional) array is a sequence of n items of the same data type that are
```
stored contiguously in computer memory and made accessible by specifying a value
of the array‚Äôs index
oA linked list is a sequence of zero or more elements called nodes, each containing
two kinds of information: some data and one or more links called pointers to other
nodes of the linked list.
Linear Data Structures: List
8
oA list is a finite sequence of data items, i.e., a collection of data items arranged
in a certain linear order.
oThe basic operations performed on this data structure are searching for,
inserting, and deleting an element.
oTwo special types of lists, stacks and queues, are particularly important.
Linear Data Structures: Stack
9
oA stack is a list in which insertions and deletions can be done only at the end.
oThis end is called the top because a stack is usually visualized not horizontally
but vertically‚Äîakin to a stack of plates whose ‚Äúoperations‚Äù it mimics very
```
closely. As a result, when elements are added to (pushed onto) a stack and
```
```
deleted from (popped off) it, the structure operates in a ‚Äúlast-in‚Äìfirst-out‚Äù (LIFO)
```
fashion‚Äî exactly like a stack of plates if we can add or remove a plate only from
the top.
Linear Data Structures: Queue
10
oA queue, on the other hand, is a list from which elements are deleted from one
```
end of the structure, called the front (this operation is called dequeue), and new
```
```
elements are added to the other end, called the rear (this operation is called
```
```
enqueue).
```
```
oConsequently, a queue operates in a ‚Äúfirst-in‚Äìfirst-out‚Äù (FIFO) fashion‚Äîakin to a
```
queue of customers served by a single teller in a bank.
Linear Data Structures: Priority Queue - Heap
11
oMany important applications require selection of an item of the highest priority
among a dynamically changing set of candidates.
oA data structure that seeks to satisfy the needs of such applications is called a
priority queue. A priority queue is a collection of data items from a totally
```
ordered universe (most often, integer or real numbers).
```
oThe principal operations on a priority queue are finding its largest element,
deleting its largest element, and adding a new element.
oA better implementation of a priority queue is based on an ingenious data
structure called the heap.
Graphs
12
oA graph is informally thought of as a collection of points in the plane called ‚Äúvertices‚Äù or
‚Äúnodes,‚Äù some of them connected by line segments called ‚Äúedges‚Äù or ‚Äúarcs.‚Äù
oFormally, a graph G = ‚ü®V , E‚ü© is defined by a pair of two sets: a finite nonempty set V of items
called vertices and a set E of pairs of these items called edges.
```
oIf these pairs of vertices are unordered, i.e., a pair of vertices (u, v) is the same as the pair (v, u),
```
we say that the vertices u and v are adjacent to each other and that they are connected by the
```
undirected edge (u, v).
```
```
oIf a pair of vertices (u, v) is not the same as the pair (v, u), we say that the edge (u, v) is directed
```
from the vertex u, called the edge‚Äôs tail, to the vertex v, called the edge‚Äôs head.
oA graph whose every edge is directed is called directed. Directed graphs are also called digraphs.
Graph Representations
13
Undirected graph Directed graph
Graph Representations
14
oGraphs for computer algorithms are usually represented in one of two ways:
o The adjacency matrix
o The adjacency lists.
Adjacency matrix Adjacency lists
```
Graphs: Weighted Graphs
```
15
```
oA weighted graph (or weighted digraph) is a graph (or di- graph) with numbers assigned to its
```
edges.
oThese numbers are called weights or costs.
oAn interest in such graphs is motivated by numerous real-world applications, such as finding the
shortest path between two points in a transportation or communication network or the traveling
salesman problem.
Weighted graph. Its weight matrix Its adjacency lists.
```
Graphs: Cycle - Acyclic
```
16
oA cycle is a path of a positive length that starts and ends at the same vertex and
does not traverse the same edge more than once.
oFor example, f, h, i, g, f is a cycle in the following graph.
o A graph with no cycles is said to be acyclic.
Trees
17
oA tree is a connected acyclic graph.
oA graph that has no cycles but is not
necessarily connected is called a forest:
each of its connected components is a
tree.
```
Trees: Rooted Trees
```
18
```
oA rooted tree is usually depicted by placing its root on the top (level 0 of the tree), the vertices
```
```
adjacent to the root below it (level 1), the vertices two edges apart from the root still below (level
```
```
2), and so on.
```
oA transformation from a free tree to a rooted tree.
```
Trees: Ordered Trees ‚Äì Binary Trees
```
19
oAn ordered tree is a rooted tree in which all the children of each vertex are ordered.
oIt is convenient to assume that in a tree‚Äôs diagram, all the children are ordered left to right.
oA binary search tree can be defined as an ordered tree in which every vertex has no more than
```
two children and each child is designated as either a left child or a right child of its parent; a binary
```
tree may also be empty.
Binary tree. Binary search tree.
Summary
o Data Structures
o Linear Data Structures
o Graphs
o Trees
20
Supportive Materials
```
o Introduction to The Design and Analysis of Algorithms (3rd Edition) by Anany
```
Levitin, Pearson.
o Introduction to Algorithms, by T.Cormen, C. Leiserson, R.Rivest and C.Stein,
MIT Press, 3rd Edition.
21
CSC 4520/6520
Design & Analysis of Algorithms
CHAPTER 2: ANALYSIS OF ALGORITHMS
A B D U L L A H BA L , P H D
1
Objectives
Recap
Data Structures
Analysis of
Algorithms
Euclid‚Äôs,Sieve,
Sequential
Search
Order of
Growth
Comparison
2
What is an algorithm?
‚ó¶ An algorithm is any well-defined computational procedure that takes
some value, or set of values, as input and produces some value, or set of values, as output
in a finite amount of time.
‚ó¶ An algorithm as a tool for solving a well-specified computational problem.
computer
problem
algorithm
input output
3
What is an algorithm?
4
```
Function: Input ‚Üí Output
```
```
Definition:
```
Step-by-step mechanical
procedure that computes a
```
function for any input
```
Summation
algorithm
12345
- 6789
19134
Input
Output
5
```
Computing gcd(m,n)?
```
```
oCompute gcd(60,24)?
```
oThe method is simply based on the definition of the greatest common divisor of m and n as the
largest integer that divides both numbers evenly.
oConsecutive integer checking algorithm
```
Step 1 Assign the value of min{m,n} to t
```
```
Step 2 Divide m by t. If the remainder is 0, go to Step 3; otherwise, go to Step 4
```
```
Step 3 Divide n by t. If the remainder is 0, return t and stop; otherwise, go to Step 4
```
Step 4 Decrease t by 1 and go to Step 2
6
```
Other methods for gcd(m,n) [cont.]
```
oMiddle-school procedure
o Step 1 Find the prime factorization of m
o Step 2 Find the prime factorization of n
o Step 3 Find all the common prime factors
```
o Step 4 Compute the product of all the common prime factors and return it as gcd(m,n)
```
oThus, for the numbers 60 and 24, we get
60 = 2 . 2 . 3 . 5
24 = 2 . 2 . 2 . 3
```
gcd(60, 24) = 2 . 2 . 3 = 12.
```
```
o gcd(571 787, 912 673)=?
```
77
Sieve of Eratosthenes
oThe algorithm starts by initializing a list of prime candidates with consecutive integers from 2 to n.
oThen, on its first iteration, the algorithm eliminates from the list all multiples of 2, i.e., 4, 6, and so on.
oThen it moves to the next item on the list, which is 3, and eliminates its multiples.
oNo pass for number 4 is needed: since 4 itself and all its multiples are also multiples of 2, they were
already eliminated on a previous pass.
oThe next remaining number on the list, which is used on the third pass, is 5.
oThe algorithm continues in this fashion until no more numbers can be eliminated from the list. The
remaining integers of the list are the primes needed.
8
Sieve of Eratosthenes
9
```
Example: Euclid‚Äôs Algorithm
```
```
oProblem: Find gcd(m,n), the greatest common divisor of two nonnegative, not
```
both zero integers m and n
```
oExamples: gcd(60,24) = 12, gcd(60,0) = 60, gcd(0,0) = ?
```
oEuclid‚Äôs algorithm is based on repeated application of equality
```
gcd(m,n) = gcd(n, m mod n)
```
until the second number becomes 0, which makes the problem trivial.
```
oExample: gcd(60,24) = gcd(24,12) = gcd(12,0) = 12
```
10
Two descriptions of Euclid‚Äôs algorithm
```
Step 1 If n = 0, return m and stop; otherwise go to Step 2
```
Step 2 Divide m by n and assign the value of the remainder to r
Step 3 Assign the value of n to m and the value of r to n. Go to Step 1.
```
Pseudocode:
```
while n ‚â† 0 do
r ‚Üê m mod n
m‚Üê n
n ‚Üê r
return m
Why study algorithms?
Theoretical importance
‚ó¶ The core of computer science
Practical importance
‚ó¶ A practitioner‚Äôs toolkit of known algorithms
```
‚ó¶ Framework for designing and analyzing algorithms for new problems (inspiration)
```
11
Analysis of algorithms
How good is the algorithm?
‚ó¶ Time efficiency
‚ó¶ Space efficiency
12
Analyzing Algorithms
ÔÅµ Analyzing an algorithm has come to mean predicting the resources that the
algorithm requires.
ÔÅµ Memory
ÔÅµ Communication bandwidth
ÔÅµ Energy consumption
ÔÅµ Computational time
13
```
Example: Insertion Sort
```
o Input: A sequence of n numbers ‚å©a1, a2, ‚Ä¶ , an‚å™.
```
A=[5,2,4,6,1,3]
```
```
o Output: A permutation (reordering) of the input sequence.
```
```
A=[1,2,3,4,5,6]
```
14
```
Example: Insertion Sort
```
o It takes two parameters:
o An array A containing the values to be sorted and the number
n of values of sort.
o The values occupy positions A[1] through A[n] of the array,
which we denote by A[1 : n].
o When the INSERTION-SORT procedure is finished, array A[1 : n]
contains the original values, but in sorted order.
15
```
Example: Insertion Sort
```
o In each iteration, the blue rectangle holds the key taken
from A[i], which is compared with the values in tan
rectangles to its left.
o Orange arrows show array values moved one position to
the right, and blue arrows indicate where the key moves to.
16
Analysis of Insertion Sort
o How long does the INSERTION-SORT procedure take?
o One way:
Run it on our computer and time how long it takes to run.
17
Analysis of Insertion Sort
o What would such a timing test tell us?
o We would find out how long insertion sort takes to run on
o our particular computer,
o on that particular input,
o under the particular implementation that we created,
o with the particular compiler or interpreter that we ran,
o with the particular libraries that we linked in,
o and with the particular background tasks that were running on our computer concurrently
```
with our timing test (such as checking for incoming information over a network).
```
18
Analysis of Insertion Sort
o If we run insertion sort again on our computer with the same input, we might even get
a different timing result.
o We need a way to predict, given a new input, how long insertion sort will take.
o Instead of timing a run, or even several runs, of insertion sort, we can determine how
long it takes by analyzing the algorithm itself.
19
o We‚Äôll examine how many times it executes each line of pseudocode and how long each
line of pseudocode takes to run.
o We‚Äôll first come up with a precise but complicated formula for the running time.
o Then, we‚Äôll distill the important part of the formula using a convenient notation that
can help us compare the running times of different algorithms for the same problem.
Analysis of Insertion Sort
20
Analysis of Insertion Sort
ÔÅµ First, let‚Äôs acknowledge that the running time depends on the input.
ÔÅµ It is not surprised that sorting a thousand numbers takes longer than sorting three
numbers.
ÔÅµ Moreover, insertion sort can take different amounts of time to sort two input arrays of
the same size, depending on how nearly sorted they already are.
ÔÅµ We need to define the terms ‚Äúrunning time‚Äù and ‚Äúinput size‚Äù more carefully.
ÔÅµ We also need to be clear about whether we are discussing the running time for an
input that elicits the worst-case behavior, the best-case behavior, or some other case.
21
Input Size
o For many problems, such as sorting or computing discrete Fourier transforms,
the most natural measure is the number of items in the input‚Äîfor example, the
number n of items being sorted.
o For many other problems, such as multiplying two integers, the best measure
of input size is the total number of bits needed to represent the input in ordinary
binary notation.
o For example, if the input to an algorithm is a graph, we usually characterize the
input size by both the number of vertices and the number of edges in the graph.
22
Running Time
o The running time of an algorithm on a particular input is the number of instructions
and data accesses executed.
o How we account for these costs should be independent of any particular computer,
but within the framework of the RAM model.
o A constant amount of time is required to execute each line of our pseudocode.
o One line might take more or less time than another line, but we‚Äôll assume that each
execution of the kth line takes ck time, where ck is a constant.
23
Running Time : The Best Case
o The best case occurs when the array is already sorted.
o The running time is thus a linear function of n.
24
Running Time : The Worst Case
ÔÅµ The worst case arises when the array is in reverse sorted order
ÔÅµ It starts out in decreasing order.
ÔÅµ The running time is a quadratic function of n.
25
Running Time : Average Case
o In some particular cases, we‚Äôll be interested in the average-case running time of an
algorithm.
o We‚Äôll see the technique of probabilistic analysis applied to various algorithms
throughout this course.
o Often, we‚Äôll assume that all inputs of a given size are equally likely.
o In practice, this assumption may be violated, but we can sometimes use a randomized
algorithm, which makes random choices, to allow a probabilistic analysis and yield an
expected running time.
26
Worst-case and Average-case Analysis
o Our analysis of insertion sort looked at both the best case, in which the input array
was already sorted, and the worst case, in which the input array was reverse sorted.
o We usually concentrate on finding only the worst-case running time, that is, the
longest running time for any input of size n.
o Why?
27
Worst-case and Average-case Analysis
Here are three reasons:
1- The worst-case running time of an algorithm gives an upper bound on the running time
for any input.
‚Ä¢ If you know it, then you have a guarantee that the algorithm never takes any longer.
‚Ä¢ This feature is especially important for real-time computing, in which operations must
complete by a deadline.
28
Worst-case and Average-case Analysis
2- For some algorithms, the worst case occurs fairly often.
‚Ä¢ For example, in searching a database for a particular piece of information,
the searching algorithm‚Äôs worst case often occurs when the information is
not present in the database.
‚Ä¢ In some applications, searches for absent information may be frequent.
29
Worst-case and Average-case Analysis
3- The ‚Äúaverage case‚Äù is often roughly as bad as the worst case.
‚Ä¢ Suppose that you run insertion sort on an array of n randomly chosen numbers.
‚Ä¢ How long does it take to determine where in subarray A[1 : i ‚Äì 1] to insert element A[i]?
‚Ä¢ On average, half the elements in A[1 : i ‚Äì 1] are less than A[i], and half the elements are
greater.
‚Ä¢ On average, therefore, A[i] is compared with just half of the subarray A[1 : i ‚Äì 1], and so ti is
about i/2. The resulting average-case running time turns out to be a quadratic function of
the input size, just like the worst-case running time.
3031
Units for Measuring Running Time
oWe can simply use some standard unit of time measurement‚Äîa second,
or millisecond, and so on‚Äîto measure the running time of a program
implementing the algorithm.
```
oDrawbacks:
```
odependence on the speed of a particular computer,
odependence on the quality of a program implementing the algorithm
and of the compiler used in generating the machine code,
othe difficulty of clocking the actual running time of the program.
oWe would like to have a metric that does not depend on these
extraneous factors.
32
oOne possible approach is to count the number of times each of the algorithm‚Äôs
operations is executed.
oThis approach is both excessively difficult and usually unnecessary.
oThat‚Äôs why, we identify the most important operation of the algorithm,
oBasic operation: The operation contributing the most to the total running time
oit is usually the most time-consuming operation in the algorithm‚Äôs innermost loop.
Units for Measuring Running Time:
Basic Operation
33
```
oMost sorting algorithms work by comparing elements (keys) of a list
```
```
being sorted with each other; for such algorithms, the basic operation
```
is a key comparison.
oAs another example, algorithms for mathematical problems typically
involve some or all of the four arithmetical operations: addition,
subtraction, multiplication, and division.
oOf the four, the most time-consuming operation is division, followed
by multiplication and then addition and subtraction, with the last two
usually considered together.
Units for Measuring Running Time:
Basic Operation
34
Input size and basic operation examples
Problem Input size measure Basic operation
Searching for key in a list of
n items
Number of list‚Äôs items, i.e. n Key comparison
Multiplication of two
matrices
Matrix dimensions or total
number of elements
Multiplication of two
numbers
Checking primality of a
given integer n
```
n‚Äôsize = number of digits (in
```
```
binary representation)
```
Division
Typical graph problem #vertices and/or edges
Visiting a vertex or
traversing an edge
35
Theoretical analysis of time efficiency
oTime efficiency is analyzed by determining the number of repetitions of the
basic operation as a function of input size
```
T(n) ‚âà copC(n)
```
running time
execution time
for basic operation
Number of times basic
operation is executed
input size
36
Theoretical analysis of time efficiency
```
T(n) ‚âà copC(n)
```
oThe formula can give a reasonable estimate of the algorithm‚Äôs running time.
oAssuming that
oHow much longer will the algorithm run if we double its input size?
37
Theoretical analysis of time efficiency
Analysis of Insertion Sort
o For each i = 2, 3, ‚Ä¶ , n, let ti denote the number of times the while loop test in line 5 is executed for
that value of i.
```
o We usually denote the running time of an algorithm on an input of size n by T(n).
```
```
o To compute T(n), the running time of INSERTION-SORT on an input of n values, we sum the
```
products of the cost and times columns, obtaining.
38
ÔÅµ We find that in the worst case, the running time of INSERTION-SORT is
ÔÅµ The running time is a quadratic function of n.
Analysis of Insertion Sort: The Worst Case
39
Order of Growth
```
o We consider only the leading term of a formula (e.g., an2), since the lower-order
```
terms are relatively insignificant for large values of n.
o We also ignore the leading term‚Äôs constant coefficient, since constant factors are
less significant than the rate of growth in determining computational efficiency for
large inputs.
o For insertion sort‚Äôs worst-case running time, when we ignore the lower-order
terms and the leading term‚Äôs constant coefficient, only the factor of n2 from the
leading term remains.
o That factor, n2, is by far the most important part of the running time.
40
Order of Growth
o For example, suppose that an algorithm implemented on a
particular machine takes n
2
/100 + 100n + 17 microseconds on an input
of size n.
o Although the coefficients of 1/100 for the n
2
term and 100 for the n
term differ by four orders of magnitude, the n
2
/100 term dominates
the 100n term once n exceeds 10,000.
o Although 10,000 might seem large, it is smaller than the population
of an average town.
o Many real-world problems have much larger input sizes.
41
Order of Growth
o To highlight the order of growth of the running time, we have a special notation
```
that uses the Greek letter Œò (theta).
```
```
o We write that insertion sort has a worst-case running time of Œò(n2)
```
```
o We also write that insertion sort has a best-case running time of Œò(n).
```
o For now, think of Œò-notation as saying ‚Äúroughly proportional when n is large,‚Äù
```
o So that Œò(n2) means ‚Äúroughly proportional to n2 when n is large‚Äù
```
```
o Œò(n) means ‚Äúroughly proportional to n when n is large.
```
42
Order of Growth
o We usually consider one algorithm to be more efficient than another if its worst-case
running time has a lower order of growth.
o Due to constant factors and lower-order terms, an algorithm whose running time has a
higher order of growth might take less time for small inputs than an algorithm whose
running time has a lower order of growth.
```
o But on large enough inputs, an algorithm whose worst-case running time is Œò(n2), for
```
example, takes less time in the worst case than an algorithm whose worst-case running time
```
is Œò(n3).
```
o Regardless of the constants hidden by the Œò-notation, there is always some number, say
```
n0, such that for all input sizes n ‚â• n0, the Œò(n2) algorithm beats the Œò(n3) algorithm in the
```
worst case.
4344
Order of growth
```
oExample: c n
```
2
oHow much faster will algorithm run on computer that is twice as fast?
oHow much longer does it take to solve problem of double input size?
45
Order of growth
```
oExample: c n
```
2
oHow much faster will algorithm run on computer that is twice as fast?
```
oAnswer: 2
```
oHow much longer does it take to solve problem of double input size?
```
oAnswer: 4
```
46
Values of some important functions as n ‚Üí ÔÇ•
o The function growing the slowest among these is the logarithmic function.
o On the other end of the spectrum are the exponential function 2n and the factorial function n!
o Both these functions grow so fast that their values become astronomically large even for rather
small values of n.
47
```
oFor example, it would take about 4 . 1010 years for a computer making a trillion (1012)
```
operations per second to execute 2100 operations.
oThough this is incomparably faster than it would have taken to execute 100! operations, it
```
is still longer than 4.5 billion (4.5 . 109) years‚Äî the estimated age of the planet Earth.
```
oThere is a tremendous difference between the orders of growth of the functions 2n and n!.
oAlgorithms that require an exponential number of operations are practical for solving only
problems of very small sizes.
Values of some important functions as n ‚Üí ÔÇ•
48
oAnother way to appreciate the qualitative difference among the orders of growth of the functions in
table is to consider how they react to, say, a twofold increase in the value of their argument n.
oThe function log2 n increases in value by just 1
log2 2n = log2 2 + log2 n = 1 + log2 n
oThe linear function increases twofold, the linearithmic function n log2 n increases slightly more than
```
twofold; the quadratic function n2 and cubic function n3 increase fourfold and eightfold, respectively.
```
```
(2n)2 = 4n2 and (2n)3 = 8n3
```
oThe value of 2n gets squared
```
because 22n = (2n)2
```
on! increases much more than that
Values of some important functions as n ‚Üí ÔÇ•
49
```
Example: Sequential search
```
oIt is reasonable to measure an algorithm‚Äôs efficiency as a function of a parameter
indicating the size of the algorithm‚Äôs input.
oThere are many algorithms for which running time depends not only on an input
size but also on the specifics of a particular input.
oSequential search: This is a straightforward algorithm that searches for a given
```
item (some search key K) in a list of n elements by checking successive elements
```
of the list until either a match with the search key is found or the list is exhausted.
50
```
Example: Sequential search
```
51
Best-case, average-case, worst-case
oFor some algorithms efficiency depends on form of input:
```
oWorst case: Cworst(n) ‚Äì maximum over inputs of size n
```
```
oBest case: Cbest(n) ‚Äì minimum over inputs of size n
```
```
oAverage case: Cavg(n) ‚Äì ‚Äúaverage‚Äù over inputs of size n
```
oNumber of times the basic operation will be executed on typical input
oNOT the average of worst and best case
oExpected number of basic operations considered as a random variable under
some assumption about the probability distribution of all possible inputs
52
Best-case, average-case, worst-case
oWorst case: When there are no matching elements or the first matching element
happens to be the last one on the list, the algorithm makes the largest number of
key comparisons among all possible inputs of size n
```
ùê∂ùë§ùëúùëüùë†ùë° (n) = n
```
oIt guarantees that for any instance of size n, the running time will not exceed
```
Cworst(n).
```
53
Best-case, average-case, worst-case
```
oBest case: An input (or inputs) of size n for which the algorithm runs the
```
fastest among all possible inputs of that size.
oThe best-case inputs for sequential search are lists of size n with their first
element equal to a search key.
```
Cbest (n) = 1
```
oThe analysis of the best-case efficiency is not nearly as important as that of
the worst-case efficiency.
54
Best-case, average-case, worst-case
oAverage case : It is clear that neither the worst-case analysis nor its best-case
counterpart yields the necessary information about an algorithm‚Äôs behavior
on a ‚Äútypical‚Äù or ‚Äúrandom‚Äù input.
oLet‚Äôs consider again sequential search. The standard assumptions are that
```
o The probability of a successful search is equal to p (0 ‚â§ p ‚â§ 1)
```
o The probability of the first match occurring in the ith position of the list
is the same for every i.
55
Best-case, average-case, worst-case
‚ó¶ Under these assumptions, we can find the average number of key comparisons
```
‚ó¶ if p = 1 (the search must be successful), the average number of key comparisons made by
```
```
sequential search is (n + 1)/2; that is, the algorithm will inspect, on average, about half of
```
the list‚Äôs elements.
```
‚ó¶ If p = 0 (the search must be unsuccessful), the average number of key comparisons will be n
```
because the algorithm will inspect all n elements on all such inputs.
56
Main Points
oBoth time and space efficiencies are measured as functions of the algorithm‚Äôs input
size.
oTime efficiency is measured by counting the number of times the algorithm‚Äôs basic
operation is executed.
oSpace efficiency is measured by counting the number of extra memory units consumed
by the algorithm.
oThe efficiencies of some algorithms may differ significantly for inputs of the same size.
o For such algorithms, we need to distinguish between the worst-case, average-case,
and best-case efficiencies.
Summary
o Analysis of Algorithms
o Euclid‚Äôs Algorithm
o Sieve Algorithm
o Sequential Search
o Running Time: The worst, best, and average case
o Order of grow comparison
57
Supportive Materials
```
o Introduction to The Design and Analysis of Algorithms (3rd Edition) by Anany
```
Levitin, Pearson.
o Introduction to Algorithms, by T.Cormen, C. Leiserson, R.Rivest and C.Stein,
MIT Press, 3rd Edition.
58
CSC 4520/6520
Design & Analysis of Algorithms
CHAPTER 2: ASYMPTOTIC ANALYSIS OF ALGORITHMS
A B D U L L A H B A L , P H D
1
Objectives
Recap
Analysis of Algorithms
Sequential Search, Euclid‚Äôs, Sieve
The best, worst, and average case
Order of Growth Comparison
Asymptotic
Order of
Growth
L‚ÄôHopital,
Stirling
Nonrecursive
Algorithms
3
Asymptotic Notations
oThe efficiency analysis framework concentrates on the order of growth of an
algorithm‚Äôs basic operation count as the principal indicator of the algorithm‚Äôs
efficiency.
oTo compare and rank such orders of growth, computer scientists use three
```
notations:
```
```
oO (big oh)
```
```
oŒò(big theta)
```
```
oŒ© (big omega)
```
4
Big-oh
```
o A function t(n) is said to be in O(g(n)),
```
```
denoted t(n) ‚àà O(g(n)),
```
```
o If t(n) is bounded above by some
```
```
constant multiple of g(n) for all large
```
n, i.e., if there exist some positive
constant c and some nonnegative
integer n0 such that
```
o t(n) ‚â§ cg(n) for all n ‚â• n0.
```
5
```
Asymptotic order of growth (O)
```
```
oO(g(n)) is the set of all functions with a lower or same order of growth as g(n) (to
```
```
within a constant multiple, as n goes to infinity).
```
```
oIn other words, g(n) must be faster than t(n) in terms of order of growth.
```
oAll the following assertions are true.
6
Big-oh
```
t(n) ‚â§ cg(n) for all n ‚â• n0.
```
oLet us formally prove following assertion:
```
100n + 5 ‚àà O(n2).
```
oIndeed,
```
100n + 5 ‚â§ 100n + n (for all n ‚â• 5) = 101n ‚â§ 101n2.
```
oThus, as values of the constants c and n0 required by the definition,
we can take 101 and 5, respectively.
7
Big-oh
o Note that the definition gives us a lot of freedom in choosing specific
values for constants c and n0.
o For example, we could also reason that
```
100n + 5 ‚â§ 100n + 5n (for all n ‚â• 1) = 105n
```
to complete the proof with c = 105 and n0 = 1.
8
Big-omega
```
o A function t(n) is said to be in Œ©(g(n)),
```
```
denoted t(n) ‚àà Œ©(g(n)),
```
```
o If t(n) is bounded below by some positive
```
```
constant multiple of g(n) for all large n, i.e.,
```
if there exist some positive constant c and
some nonnegative integer n0 such that
```
o t(n) ‚â• cg(n) for all n ‚â• n0.
```
9
```
Asymptotic order of growth (Œ©)
```
```
oThe second notation, Œ©(g(n)), stands for the set of all functions with a higher or
```
```
same order of growth as g(n) (to within a constant multiple, as n goes to infinity).
```
```
oIn other words, g(n) must be slower than t(n) in terms of order of growth.
```
oFor example:
10
Big-omega
o Here is an example of the formal proof that
```
n3 ‚àà Œ©(n2)
```
n3 ‚â• n2 for all n ‚â• 0,
o i.e.,we can select c=1 and n0 =0
11
Big-theta
```
o A function t(n) is said to be in Œò(g(n)),
```
```
denoted t(n) ‚àà Œò(g(n)),
```
```
o If t(n) is bounded both above and below by
```
```
some positive constant multiples of g(n) for
```
all large n, i.e., if there exist some positive
constants c1 and c2 and some nonnegative
integer n0 such that
```
o c2g(n) ‚â§ t(n) ‚â§ c1g(n) for all n ‚â• n0.
```
12
```
Asymptotic order of growth (Œò)
```
```
oŒò(g(n)) is the set of all functions that have the same order of
```
```
growth as g(n) (to within a constant multiple, as n goes to
```
```
infinity).
```
```
oThus, every quadratic function an2 + bn + c with a > 0 is in Œò(n2).
```
13
Big-theta
14
Establishing order of growth using the definition
```
Definition: f(n) is in O(g(n)) if order of growth of f(n) ‚â§ order of growth of g(n)
```
```
(within constant multiple),
```
i.e., there exist positive constant c and non-negative integer n0 such that
```
f(n) ‚â§ cg(n) for every n ‚â• n0
```
```
Example 1: 10n is O(n2)
```
```
Solution: 10n ‚â§ n2 for n ‚â• 10
```
```
c=1 and n0=10
```
```
Example 2: 5n+20 is O(n)
```
```
Solution: 5n+20 ‚â§ 10n for n ‚â• 4
```
```
c=10 and n0=4
```
Establishing order of growth using limits
```
lim t(n)/g(n) =
```
```
0 order of growth of T(n) < order of growth of g(n)
```
```
c > 0 order of growth of T(n) = order of growth of g(n)
```
```
‚àû order of growth of T(n) > order of growth of g(n)
```
n‚Üí‚àû
o A much more convenient method for comparing the orders of growth of two specific functions is
based on computing the limit of the ratio of two functions in question.
```
o Note that the first two cases mean that t(n) ‚àà O(g(n)), the last two mean that t(n) ‚àà Œ©(g(n)), and
```
```
the second case means that t(n) ‚àà Œò(g(n)).
```
```
Examples: 1) 10n vs. n2
```
```
2) n(n+1)/2 vs. n2
```
```
O(g(n))
```
```
Œò(g(n))
```
```
Œ©(g(n))
```
16
L‚ÄôH√¥pital‚Äôs rule and Stirling‚Äôs formula
```
L‚ÄôH√¥pital‚Äôs rule: If limn¬Æ¬• f(n) = limn¬Æ¬• g(n) = ¬• and
```
the derivatives f¬¥,and g¬¥ exist, then
```
f(n)
```
```
g(n)
```
lim
n¬Æ¬•
=
```
f ¬¥(n)
```
```
g ¬¥(n)
```
lim
n¬Æ¬•
o The limit-based approach is often more convenient.
o It can take advantage of the powerful calculus techniques developed for
computing limits, such as L‚ÄôHopital‚Äôs rule
17
L‚ÄôH√¥pital‚Äôs rule and Stirling‚Äôs formula
Stirling‚Äôs formula:
```
Examples: 1) log n vs. n
```
```
2) n! vs. 2n
```
18
L‚ÄôH√¥pital‚Äôs rule and Stirling‚Äôs Examples
19
```
L‚ÄôH√¥pital‚Äôs rule and Stirling‚Äôs Examples (Cont.)
```
20
```
L‚ÄôH√¥pital‚Äôs rule and Stirling‚Äôs Examples (Cont.)
```
21
Orders of growth of some important functions
oAll logarithmic functions loga n belong to the same class:
```
Q(log n) no matter what the logarithm‚Äôs base a > 1 is
```
oAll polynomials of the same degree k belong to the same class:
```
aknk + ak-1nk-1 + ‚Ä¶ + a0 √é Q(nk)
```
oExponential functions an have different orders of growth for different a‚Äôs
```
oOrder log n < order na (a>0) < order an < order n! < order nn
```
22
Basic asymptotic efficiency classes
1 constant
log n logarithmic
n linear
n log n n-log-n or
linearithmic
n2 quadratic
n3 cubic
2n exponential
n! factorial
o There are still infinitely many such classes.
o For example, the exponential functions an
have different orders of growth for
different values of base a.
o Therefore, it may come as a surprise that
the time efficiencies of a large number of
algorithms fall into only a few classes.
23
Basic asymptotic efficiency classes
```
https://www.bigocheatsheet.com/24
```
```
Example: The Power of Exponential Growth
```
1.Take a piece of paper and fold it in half. How many layers is
```
it? (2)
```
```
2.Fold it in half again. How many layers is it now? (4)
```
```
3.Fold it in half again. How many layers now? (8)
```
4.What do you expect to happen if you fold it in half a fourth
```
time? (16 layers)
```
```
5.Can you identify the pattern? (if n is the number of times
```
```
you folded, the pattern is 2^n, or 2 x 2 x 2 x 2 x‚Ä¶ n times)
```
If you fold it in half 42 times, what will be its thickness?
25
```
Example: The Power of Exponential Growth
```
o Thickness of a paper = 0.1 mm
o 2^42 x 0.1 mm = 439,804 kilometers
o The average distance from the Earth to the Moon is about 384,400 km
26
Time efficiency of
NONRECURSIVE
algorithms
27
Time efficiency of nonrecursive algorithms
General Plan for Analysis
oDecide on parameter n indicating input size
oIdentify algorithm‚Äôs basic operation
oDetermine worst, average, and best cases for input of size n
oSet up a sum for the number of times the basic operation is executed
oSimplify the sum using standard formulas and rules
28
Useful summation formulas and rules
oIn particular, we use especially frequently two basic rules of sum manipulation
o Two summation formulas
29
Example 1: Maximum element
30
```
Example 1: Maximum element (Cont.)
```
o The obvious measure of an input‚Äôs size here is the number of elements in the array, i.e.,
n.
o The operations that are going to be executed most often are in the algorithm‚Äôs for loop.
o There are two operations in the loop‚Äôs body: the comparison A[i] > maxval and the
assignment maxval ‚Üê A[i].
o Which of these two operations should we consider basic?
o Since the comparison is executed on each repetition of the loop and the assignment is
not, we should consider the comparison to be the algorithm‚Äôs basic operation.
```
o Note that the number of comparisons will be the same for all arrays of size n; therefore,
```
in terms of this metric, there is no need to distinguish among the worst, average, and
best cases here.
31
```
o Let us denote C(n) the number of times this comparison is executed and try to find a formula
```
expressing it as a function of size n.
o The algorithm makes one comparison on each execution of the loop, which is repeated for each
value of the loop‚Äôs variable i within the bounds 1 and n ‚àí 1, inclusive.
```
o Therefore, we get the following sum for C(n):
```
o This is an easy sum to compute because it is nothing other than 1 repeated n ‚àí 1 times.
Thus,
```
Example 1: Maximum element (Cont.)
```
32
Example 2: Element uniqueness problem
33
```
Example 2: Element uniqueness problem (Cont.)
```
o The natural measure of the input‚Äôs size here is again n, the number of elements in the array.
```
o Since the innermost loop contains a single operation (the comparison of two elements), we
```
should consider it as the algorithm‚Äôs basic operation.
o Note, however, that the number of element comparisons depends not only on n but also on
whether there are equal elements in the array and, if there are, which array positions they
occupy.
o We will limit our investigation to the worst case only.
o By definition, the worst case input is an array for which the number of element comparisons
```
Cworst(n) is the largest among all arrays of size n.
```
34
```
Example 2: Element uniqueness problem (Cont.)
```
o There are two kinds of worst-case inputs for which the algorithm does not exit the
loop prematurely:
```
o 1) Arrays with no equal elements
```
```
o 2) Arrays in which the last two elements are the only pair of equal elements.
```
o One comparison is made for each repetition of the innermost loop, i.e., for each
```
value of the loop variable j between its limits i + 1 and n ‚àí 1;
```
o This is repeated for each value of the outer loop, i.e., for each value of the loop
variable i between its limits 0 and n ‚àí 2.
35
```
Example 2: Element uniqueness problem (Cont.)
```
36
Example 3: Matrix multiplication
37
```
Example 3: Matrix multiplication (Cont.)
```
38
```
Example 3: Matrix multiplication (Cont.)
```
o We consider multiplication as the basic operation.
```
o Let us set up a sum for the total number of multiplications M(n) executed
```
by the algorithm.
o Since this count depends only on the size of the input matrices, we do not
have to investigate the worst-case, average-case, and best-case
efficiencies separately.
o Obviously, there is just one multiplication executed on each repetition of
the algorithm‚Äôs innermost loop, which is governed by the variable k
ranging from the lower bound 0 to the upper bound n ‚àí 1.
39
```
Example 3: Matrix multiplication (Cont.)
```
o Therefore, the number of multiplications made for every pair of specific
values of variables i and j is
```
o The total number of multiplications M(n) is expressed by the following
```
triple sum:
40
```
Example 3: Matrix multiplication (Cont.)
```
o If we now want to estimate the running time of the algorithm on a particular machine, we can do
it by the product
where cm is the time of one multiplication on the machine in question.
o We would get a more accurate estimate if we took into account the time spent on the additions,
```
too:
```
where ca is the time of one addition.
o Note that the estimates differ only by their multiplicative constants and not by their order of
growth.
41
Example 4: Counting binary digits
o Find the number of binary digits in the binary representation of a positive
decimal integer.
42
Example 4: Counting binary digits
o The time complexity is proportional to the number of digits.
o If there are 3,000 digits in the number, the while loop will iterate 3,000 times.
o How many digits are there in n, asymptotically?
o When we divide n by 2, after k division we will get 1 as output.
o In other words,
!
"!
=1
```
o Taking a logarithm, we have k=log2(n)
```
o Algorithm executes k times to find out the digit number.
```
o The above shows that the algorithm‚Äôs time complexity is Œò(log2(n)).
```
Summary
```
o Asymptotic Order of Growth (Big oh, Omega, and Theta)
```
o L‚ÄôHopital‚Äôs rules,Stirling
o Nonrecursive Algorithms
o Maximum Element
o Element Uniqueness
o Matrix multiplication
o Counting Binary Digits
Supportive Materials
```
o Introduction to The Design and Analysis of Algorithms (3rd Edition) by Anany
```
Levitin, Pearson.
o Introduction to Algorithms, by T.Cormen, C. Leiserson, R.Rivest and C.Stein,
MIT Press, 3rd Edition.
CSC 4520/6520
Design & Analysis of Algorithms
CHAPTER 2: RECURSIVE ALGORITHMS
1
Abdullah Bal, PhD
Objectives
Recap
Asymptotic Order of Growth
L‚ÄôHopital
Stirling
Nonrecursive Algorithms
- Maximum element
-Element Uniqueness
-Matrix Multiplications
-Counting Binary Digits
Analysis of
Loops
What is
recursion
and
recursive
algorithm?
Memoization
2
Analysis of Algorithms: For Loop
```
for (i=0, i<n, i++)
```
```
{
```
```
Statements;
```
```
}
```
n+1
n
```
Time complexity: Œò (n)
```
o Incrementing for loop
3
Analysis of Algorithms: For Loop
```
for (i=n, i>0, i--)
```
```
{
```
```
Statements;
```
```
}
```
n+1
n
```
Time complexity: Œò (n)
```
o Decrementing for loop
4
Analysis of Algorithms: For Loop
```
for (i = 1, i < n, i=i+2)
```
```
{
```
```
Statements;
```
```
}
```
n/2
```
Time complexity: Œò (n)
```
5
Analysis of Algorithms: For Loop
```
m = 0
```
```
for (i = 1, m <=n, i ++)
```
```
{
```
```
m=m+i;
```
```
}
```
```
o When i=k, m becomes greater than n (m>n)
```
```
o m=1+2+3+‚Ä¶..+k=k(k+1)/2
```
```
o k(k+1)/2 >n
```
o k> ùëõ
```
Time complexity: Œò ( ùëõ)
```
i m
1 0+1
2 1+2
3 1+2+3
4 1+2+3+4
. .
. .
k 1+2+3+4+‚Ä¶‚Ä¶.+k
6
Analysis of Algorithms: For Loop
```
for (i=n, i>=1, i=i/2)
```
```
{
```
```
Statements;
```
```
}
```
```
i =
```
"
#!
= 1
```
n = 2k
```
```
Time complexity: Œò (log2n)
```
i
n
n/2
n/22
n/23
.
.
n/2k
7
Analysis of Algorithms: For Loop
```
for (i=1, i<=n, i++)
```
```
{
```
```
Statements;
```
```
}
```
```
i=1+1+1+1+‚Ä¶‚Ä¶‚Ä¶+1=n
```
```
k = n
```
```
Time complexity: Œò (n)
```
```
for (i=1, i<n, i=i*2)
```
```
{
```
```
Statements;
```
```
}
```
```
i=1*2*2*2*2*2*‚Ä¶*2=n
```
2k = n
```
Time complexity: Œò (log2n)
```
8
Analysis of Algorithms: For Loops
```
for (i = 0, i < n, i++)
```
```
{
```
```
Statements;
```
```
}
```
n
```
Time complexity: Œò (n)
```
```
for (i = 0, i < n, i++)
```
```
{
```
```
Statements;
```
```
}
```
n
```
Total = 2n
```
9
Analysis of Algorithms: Nested For Loops
```
for (i = 0, i < n, i ++)
```
```
{
```
```
for (j = 0, j < n, j ++)
```
```
{
```
```
Statements;
```
```
}
```
```
}
```
n+1
```
Time complexity: Œò (n2)
```
```
n * (n+1)
```
n * n
11
Analysis of Algorithms: For Loops
```
for (i = 0, i < n, i++)
```
```
for (i = 0, i < n, i=i+2)
```
```
for (i = n, i > n, i--)
```
```
for (i = 1, i < n, i=i*2)
```
```
for (i = 1, i < n, i=i*3)
```
```
for (i = n, i > 1, i=i/2)
```
```
Œò (n)
```
```
n/2 Œò (n)
```
```
Œò (n)
```
```
Œò (log 2 n)
```
```
Œò (log 3 n)
```
```
Œò (log 2 n)
```
14
Analysis of Algorithms: If & While
```
for (i = 0, i < n, i ++)
```
```
{
```
```
Statements;
```
```
}
```
n+1
```
Time complexity: Œò (n)
```
n
```
i=0
```
```
while ( i < n)
```
```
{
```
```
Statements;
```
```
i++;
```
```
}
```
1
```
Time complexity: Œò (n)
```
n+1
n
```
f(n)= 3n+2
```
n
```
f(n)= 2n+1
```
15
Analysis of Algorithms: If & While
```
i = 1;
```
```
m = 1;
```
```
while (m <n)
```
```
{
```
```
Statements;
```
```
m=m+i;
```
```
i++;
```
```
}
```
```
When i=k, m becomes equal to n (m=n)
```
```
m=n=1+2+3+‚Ä¶..+k=k(k+1)/2
```
```
k(k+1)/2 >n
```
k> ùëõ
```
Time complexity: Œò ( ùëõ)
```
i m
1 1
2 1+1=2
3 2+2
4 2+2+3
5 2+2+3+4
. .
k 2+2+3+4+‚Ä¶‚Ä¶.+k
17
Analysis of Algorithms: If & While
```
function unknown(n)
```
```
{
```
```
if (n<7)
```
```
{
```
```
printf(‚Äú%d‚Äù,n);
```
```
}
```
else
```
{
```
```
for (i = 0, i < n, i++)
```
```
{
```
```
printf(‚Äú%d‚Äù,i);
```
```
}
```
```
}
```
```
}
```
1
n
```
Time complexity: Œò (n)
```
19
RECURSION
20
What is recursion?
Recursion is a technique in programming where a function calls itself in order to
solve a problem.
```
def biggest():
```
```
biggest()
```
21
Recursion in shapes
Sierpinski‚Äôs triangles
```
The recursive centaur (Mythical creature).
```
Image by Joseph Parker.
```
https://inventwithpython.com/recursion/chapter1.html 22
```
Recursive cartoon
```
https://giphy.com/gifs/homer-simpson-the-simpsons-3ov9jQX2Ow4bM5xxuM
```
Bart simpson-Homer simpson
23
Definition of recursion nature
o If problems are solved by reducing them to smaller problems of
the same form, it is said to have recursion nature
o It can be a powerful tool for solving problems that have a
recursive structure
24
Factorial has a recursive nature?
n!= n x
```
n!= n x (n-1)!
```
```
(n-3) x ‚Ä¶..2 x 1(n-1) x (n-2) x
```
25
Factorial has a recursive nature
5! = 5 x 4 x 3 x 2 x 1 = 120
4! = 4 x 3 x 2 x 1 = 24
5! = 5 x 4! = 120
26
Recursive factorial function
```
n! = n x (n-1)!
```
```
def factorial(n):
```
```
return n * factorial(n-1)
```
```
print(factorial(5))
```
27
Recursive factorial function
```
RecursionError Traceback (most recent call last)
```
```
Untitled-1.ipynb Cell 3' in <cell line: 4>()
```
‚Ä¶.
```
[... skipping similar frames: factorial at line 2 (2970 times)]
```
```
Untitled-1.ipynb Cell 3' in factorial(number)
```
```
1 def factorial(number):
```
```
----> 2 return number * factorial(number-1)
```
```
RecursionError: maximum recursion depth exceeded
```
28
Recursive factorial function
5! = 5 x 4 x 3 x 2 x 1 x 0 x -1 x -2 x‚Ä¶..
```
RecursionError: maximum recursion depth exceeded
```
‚Ä¢ A stack oveflow is when a recursive function gets out of
control and does not stop recursing
29
Base Case - Recursive Case
```
def factorial(number):
```
if number == 1:
return 1
```
return number * factorial(number-1)
```
```
print(factorial(5))
```
‚Ä¢ Recursive function must always have at least
one Base Case and one Recursive Case.
```
Hint: 1!=1
```
# Base Case
# Recursive Case
30
Visualizing Recursion
```
fact (5)
```
```
num = 5
```
Because num != 1
```
return 5*fact(4);
```
```
fact (4)
```
```
num = 4
```
Because num != 1
```
return 4*fact(3);
```
```
fact (3)
```
```
num = 3
```
Because num != 3
```
return 3*fact(2);
```
```
fact (2)
```
```
num = 2
```
Because num != 1
```
return 2*fact(1);
```
```
fact (1)
```
```
num = 1
```
Because num == 1
```
return 1;
```
```
fact (5)= ?
```
```
def fact(num):
```
if num == 1:
return 1
```
return num * fact(num-1)
```
31
Recursion-Limitations
```
factorial(3000)
```
‚Ä¢ Python has a limit of 2970 function calls.
```
RecursionError: maximum recursion depth exceeded in comparison
```
32
Iterative factorial function
```
def factorial(number):
```
```
fact=1
```
```
for i in range(1,number+1):
```
```
fact = fact * i
```
return fact
```
print(factorial(5))
```
33
Recursion or Iteration?
o In recursion, the function calls itself until it reaches the base case,
o In iteration, the program executes a loop until a condition is met.
o Memory Usage:
o Recursion can be memory-intensive, as each recursive call adds a new
entry to the call stack, which can potentially lead to a stack overflow
error.
o On the other hand, iteration typically uses less memory because it
doesn't add new entries to the call stack.
34
Recursion or Iteration?
o Code Complexity:
o Recursive solutions can be easier to read and understand for problems
that have a recursive structure,
o Iterative solutions may require more complex code.
o Performance:
o Recursive solutions can be slower than iterative solutions due to the
overhead of function calls and the potential for stack overflow errors.
o However, some problems may be better suited for recursion and may
have faster recursive solutions than iterative ones.
35
```
Activity: Computing Power
```
```
oWrite a recursive function that takes in a number (x) and an exponent (n) and
```
returns the result of ùë•
!
```
def power(x , n):
```
if n == 0:
return 1 # Base Case
```
return x * power(x,n-1) # Recursive Case
```
36
What is excessive repetition in recursion?
oWhen recursive algorithms are designed carelessly, it can lead to very inefficient
and unacceptable solutions.
o For example, let us consider the Fibonacci sequence
1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 . . . . .
```
fib(n) = fib(n-1) + fib(n-2)
```
```
fib(1) and fib(2) = 1
```
37
Recursive function for Fibonacci
```
def fib(number):
```
if number == 1 or number == 2:
return 1 # Base case
```
else:
```
```
return fib(number-2) + fib(number -1) # Recursive case
```
38
Excessive repetition in recursion
oRecursive Fibonacci ends up repeating the same computation numerous times.
```
oLet nk be the number of recursive calls by fib(k)
```
‚Ä¢ n0 = 1
‚Ä¢ n1 = 1
‚Ä¢ n2 = n1 + n0 + 1 = 1 + 1 + 1 = 3
‚Ä¢ n3 = n2 + n1 + 1 = 3 + 1 + 1 = 5
‚Ä¢ n4 = n3 + n2 + 1 = 5 + 3 + 1 = 9
‚Ä¢ n5 = n4 + n3 + 1 = 9 + 5 + 1 = 15
‚Ä¢ n6 = n5 + n4 + 1 = 15 + 9 + 1 = 25
‚Ä¢ n7 = n6 + n5 + 1 = 25 + 15 + 1 = 41
‚Ä¢ n8 = n7 + n6 + 1 = 41 + 25 + 1 = 67
o Note that nk at least doubles every other time
39
Visualizing Recursive Fibonacci
```
fib(5)
```
```
fib(4) fib(3)
```
```
fib(3) fib(2) fib(2) fib(1)
```
1 1 1
```
fib(2) fib(1)
```
1 1
40
Visualizing Recursive Fibonacci
```
fib(5)
```
```
fib(4) fib(3)
```
```
fib(3) fib(2) fib(2) fib(1)
```
1 1 1
```
fib(2) fib(1)
```
1 1
41
Recursive Fibonacci with Memoization
```
Fib_Cache = {}
```
```
def fib(number):
```
if number in Fib_Cache:
return Fib_Cache[number]
if number == 1 or number == 2:
return 1 # Base case
```
else:
```
```
Fib_Cache[number] = fib(number-2) + fib(number -1) # Recursive case
```
return Fib_Cache[number]
4243
Recursive analysis of factorial?
0! = 1
```
Recursive definition of n!: F(n) = F(n-1) √ó n for n ‚â• 1
```
Summary
```
o Analysis of Loops (For-If-While)
```
```
o Recursion (Base Case, recursive case)
```
o Memoization
44
Supportive Materials
```
o Introduction to The Design and Analysis of Algorithms (3rd Edition) by Anany
```
Levitin, Pearson.
o Introduction to Algorithms, by T.Cormen, C. Leiserson, R.Rivest and C.Stein,
MIT Press, 3rd Edition.
45
CSC 4520/6520
Design & Analysis of Algorithms
CHAPTER 2: ANALYSIS OF RECURSIVE ALGORITHMS
1
Abdullah Bal, PhD
Objectives
Recap
-Analysis of Loops
-Recursive algorithms
Recursive
Algorithm
Analysis
Backward
substitutions
Recursive-
Iterative
comparison
23
Plan for Analysis of Recursive Algorithms
oDecide on a parameter indicating an input‚Äôs size.
oIdentify the algorithm‚Äôs basic operation.
oCheck whether the number of times the basic operation is executed may vary on different inputs of
the same size.
oIf it may, the worst, average, and best cases must be investigated separately.
oSet up a recurrence relation with an appropriate initial condition expressing the number of times
the basic operation is executed.
```
oSolve the recurrence (or, at the very least, establish its solution‚Äôs order of growth) by backward
```
substitutions or another method.
4
Example 1: Recursive analysis of n!
0! = 1
```
Recursive definition of n!: F(n) = F(n-1) √ó n for n ‚â• 1
```
5
```
Solving the recurrence for M(n)
```
o To determine a solution uniquely, we need an initial condition that tells us the value with which
the sequence starts.
o We can obtain this value by inspecting the condition that makes the algorithm stop its recursive
```
calls:
```
if n = 0 return 1.
o We are dealing here with two recursively defined functions.
```
o The first is the factorial function F(n) itself; it is defined by the recurrence
```
```
F(n)=F(n‚àí1).n for every n>0,
```
```
F(0)=1.
```
```
o The second is the number of multiplications M(n) needed to compute F(n) by the recursive
```
algorithm.
6
Backward Substitutions
o It will be more useful to arrive at it in a systematic fashion.
o We use what can be called the method of backward substitutions.
```
o The method‚Äôs idea (and the reason for the name) is immediately clear from the way it applies to solving
```
our particular recurrence:
```
M(n) = M(n ‚àí 1) + 1 substitute M(n ‚àí 1) = M(n ‚àí 2) + 1
```
```
=[M(n‚àí2)+1]+1=M(n‚àí2)+2 substitute M(n‚àí2)=M(n‚àí3)+1
```
```
= [M(n ‚àí 3) + 1] + 2 = M(n ‚àí 3) + 3.
```
o After inspecting the first three lines, we see an emerging pattern, which makes it possible to predict not
```
only the next line (what would it be?) but also a general formula for the pattern:
```
```
M(n) = M(n ‚àí i) + i
```
7
```
Solving the recurrence for M(n)
```
o Let us take advantage of the initial condition given.
o Since it is specified for n = 0, we have to substitute i = n in the pattern‚Äôs
formula to get the ultimate result of our backward substitutions:
```
M(n) = M(n ‚àí 1) + 1 = . . . = M(n ‚àí i) + i = . . . = M(n ‚àí n) + n = n
```
o The simple iterative algorithm that accumulates the product of n
consecutive integers requires the same number of multiplications.
8
Example 2: Backward Substitutions
```
Problem: What is the time complexity of the following recursive
```
algorithm?
```
T(n) = T(n ‚àí 1) + n
```
```
T(1)=1
```
9
Example 2: Backward Substitutions
```
Solution: Guess the pattern (start with given recurrence)
```
```
T(n) = T(n ‚àí 1) + n
```
```
substitute T(n ‚àí 1) = T(n ‚àí 2) +( n-1)
```
```
T(n)=T(n-2) + (n-1) + n
```
```
substitute T(n ‚àí 2) = T(n ‚àí 3) + (n-2)
```
```
T(n)=T(n-3) + (n-2) + (n-1) + n
```
‚Ä¶.
```
T(n)=T(n-i) + (n-i+1) + (n-i+2) +‚Ä¶‚Ä¶‚Ä¶+ n
```
n-i=1 √† i=n-1
```
T(n)=T(1) + 2 + 3 +‚Ä¶‚Ä¶‚Ä¶+ n
```
```
T(n)=1+ 2 + 3 +‚Ä¶‚Ä¶‚Ä¶+ n=n(n+1)/2
```
```
Time complexity: Œò(n2)
```
10
Example 3: Backward Substitutions
```
Problem: What is the time complexity of the following recursive
```
algorithm?
```
T(n) = n*T(n ‚àí 1)
```
```
T(1)=1
```
11
Example 3: Backward Substitutions
```
Solution: Guess the pattern (start with given recurrence)
```
```
T(n) = n * T(n ‚àí 1)
```
```
substitute T(n ‚àí 1) = (n-1) * T(n ‚àí 2)
```
```
T(n)= n * (n-1) * T(n-2)
```
```
substitute T(n ‚àí 2) = (n-2) * T(n ‚àí 3)
```
```
T(n)= n * (n-1) * (n-2) * T(n-3)
```
‚Ä¶.
```
T(n)= n * (n-1) * (n-2) * ‚Ä¶‚Ä¶*(n-i+1) *T(n-i)
```
n-i=1 √† i=n-1
```
T(n)=T(1) * 2 * 3 *‚Ä¶‚Ä¶‚Ä¶* (n-1) * n
```
```
T(n)=n!
```
```
Time complexity: Œò(n!)
```
12
Example 4: Backward Substitutions
```
Problem: What is the time complexity of the following recursive
```
algorithm?
```
T(n) = T(n /3) + 1
```
```
T(1)=1
```
13
Example 4: Backward Substitutions
```
Solution: Guess the pattern (start with given recurrence)
```
```
T(n) = T(n /3) + 1
```
```
substitute T(n /3) = T(n/32) +1
```
```
T(n)=T(n/32) +1 +1= T(n/32) +2
```
```
substitute T(n/32) = T(n/33) +1
```
```
T(n)=T(n/33) +1 +2 = T(n/33) +3
```
‚Ä¶.
```
T(n)=T(n/3i) +i
```
n/3i =1 √† i=log3n
```
T(n)=1+ log3n
```
```
Time complexity: Œò(log3n)
```
14
Example 5: Backward Substitutions
```
Problem: What is the time complexity of the following recursive
```
algorithm?
```
T(n) = 2T(n ‚àí 1) + 1
```
```
T(1)=0
```
15
Example 5: Backward Substitutions
```
Solution: Guess the pattern (start with given recurrence)
```
```
T(n) = 2T(n ‚àí 1) + 1
```
```
substitute T(n ‚àí 1) = 2T(n ‚àí 2) +1
```
```
T(n)=2(2T(n-2) + 1) + 1 = 22 T(n-2) + 2 +1
```
```
substitute T(n ‚àí 2) = 2T(n ‚àí 3) + 1
```
```
T(n)= 22(2T(n ‚àí 3) + 1) + 2 +1=23 T(n-3) + 22 + 2 +1
```
‚Ä¶.
```
T(n)=2i T(n-i) + 2(i-1) + ‚Ä¶‚Ä¶.. + 22 + 2+ 1
```
```
T(n)=2i T(n-i) + 2i -1
```
n-i=1 √† i=n-1
```
T(n)= 2n-1 -1=
```
!!
!
-1
```
Time complexity: Œò(2n )
```
16
Example 6: The Tower of Hanoi Puzzle
oWe have n disks of different sizes that can
slide onto any of three pegs.
oInitially, all the disks are on the first peg in
order of size, the largest on the bottom and the
smallest on top.
oThe goal is to move all the disks to the third
peg, using the second one as an auxiliary, if
necessary.
oWe can move only one disk at a time, and it is
forbidden to place a larger disk on top of a
smaller one.
```
https://colab.research.google.com/drive/1t98NL6yk-28cjZ8NOHRJquQQYJhXPU9m?usp=sharing17
```
```
Example 6: The Tower of Hanoi Puzzle (Cont.)
```
oThe problem has an elegant recursive solution.
```
oTo move n > 1 disks from peg 1 to peg 3 (with peg 2
```
```
as auxiliary), we first move recursively n ‚àí 1 disks
```
```
from peg 1 to peg 2 (with peg 3 as auxiliary), then
```
move the largest disk directly from peg 1 to peg 3,
and, finally, move recursively n ‚àí 1 disks from peg 2
```
to peg 3 (using peg 1 as auxiliary).
```
oOf course, if n = 1, we simply move the single disk
directly from the source peg to the destination peg.
18
```
Example 6: The Tower of Hanoi Puzzle (Cont.)
```
oApply the general plan outlined above to the Tower of Hanoi problem.
oThe number of disks n is the obvious choice for the input‚Äôs size indicator, and so is moving one
disk as the algorithm‚Äôs basic operation.
```
oClearly, the number of moves M(n) depends on n only, and we get the following recurrence
```
equation for it:
```
M(n)=M(n‚àí1)+1+M(n‚àí1) for n>1
```
```
oWith the obvious initial condition M(1) = 1, we have the following recurrence relation for the
```
```
number of moves M(n):
```
```
M(n)=2M(n‚àí1)+1 for n>1
```
```
M(1)=1.
```
19
```
Example 6: The Tower of Hanoi Puzzle (Cont.)
```
oWe solve this recurrence by the same method of backward substitutions:
```
M(n) = 2M(n ‚àí 1) + 1 sub. M(n ‚àí 1) = 2M(n ‚àí 2) + 1
```
```
=2[2M(n‚àí2)+1]+1=22M(n‚àí2)+2+1 sub. M(n‚àí2)=2M(n‚àí3)+1
```
```
=22[2M(n‚àí3)+1]+2+1=23M(n‚àí3)+22 +2+1.
```
oThe pattern of the first three sums on the left suggests that the next one will be
```
o 24M(n ‚àí 4) + 23 + 22 + 2 + 1,
```
o Generally, after i substitutions, we get
```
M(n) = 2iM(n ‚àí i) + 2i‚àí1 + 2i‚àí2 + . . . + 2 + 1
```
20
```
Example 6: The Tower of Hanoi Puzzle (Cont.)
```
```
M(n) = 2iM(n ‚àí i) + 2i‚àí1 + 2i‚àí2 + . . . + 2 + 1 = 2iM(n ‚àí i) + 2i ‚àí 1
```
oSince the initial condition is specified for n = 1, which is achieved for i = n ‚àí 1,
oWe get the following formula for the solution to recurrence:
```
M(n)=2n‚àí1 M(n‚àí(n‚àí1))+2n‚àí1 ‚àí1 =2n‚àí1 M(1)+2n‚àí1 ‚àí1=2n‚àí1 +2n‚àí1 ‚àí1=2n ‚àí1
```
21
```
Example 6: The Tower of Hanoi Puzzle (Cont.)
```
oWe get the following formula for the solution to recurrence:
```
M(n)=2n‚àí1 M(n‚àí(n‚àí1))+2n‚àí1 ‚àí1 =2n‚àí1 M(1)+2n‚àí1 ‚àí1=2n‚àí1 +2n‚àí1 ‚àí1=2n ‚àí1.
```
```
oWe have an exponential algorithm Œò(2
```
n
```
), which will run for an
```
unimaginably long time even for moderate values of n.
22
Example 7: Fibonacci numbers
o The Fibonacci numbers:
0, 1, 1, 2, 3, 5, 8, 13, 21, ‚Ä¶
o The Fibonacci recurrence:
```
F(n) = F(n-1) + F(n-2) for n>1
```
o Two initial conditions
```
F(0) = 0 , F(1) = 1
```
23
Example 7: Fibonacci numbers recursive
```
o The algorithm‚Äôs basic operation is clearly addition, so let A(n) be the number of additions
```
```
performed by the algorithm in computing F(n).
```
```
o The numbers of additions needed for computing F(n ‚àí 1) and F(n ‚àí 2) are A(n ‚àí 1) and
```
```
A(n ‚àí 2), respectively.
```
o The algorithm needs one more addition to compute their sum.
24
```
Example 7: Fibonacci numbers recursive(Cont.)
```
```
o Thus, we get the following recurrence for A(n):
```
```
A(n)=A(n‚àí1)+A(n‚àí2)+1 for n>1
```
```
A(0) = 0, A(1) = 0
```
o The recurrence
```
A(n) ‚àí A(n ‚àí 1) ‚àí A(n ‚àí 2) = 1 is quite similar to recurrence
```
```
F(n) ‚àí F(n ‚àí 1) ‚àí F(n ‚àí 2) = 0,
```
o But its right-hand side is not equal to zero.
o Such recurrences are called inhomogeneous.
```
o There are general techniques for solving inhomogeneous recurrences (discrete mathematics),
```
o For this particular recurrence, a special trick leads to a faster solution.
25
```
Example 7: Fibonacci numbers recursive(Cont.)
```
o We can reduce our inhomogeneous recurrence to a homogeneous one by rewriting it as
```
[A(n)+1]‚àí[A(n‚àí1)+1]‚àí[A(n‚àí2)+1]=0 and substituting
```
```
B(n) = A(n) + 1
```
```
B(n)‚àíB(n‚àí1)‚àíB(n‚àí2)=0,
```
```
B(0) = 1, B(1) = 1.
```
o This homogeneous recurrence can be solved.
```
o But it can actually be avoided by noting that B(n) is, in fact, the same recurrence as F(n)
```
```
except that it starts with two 1‚Äôs and thus runs one step ahead of F (n).
```
```
B(n) = F (n + 1)
```
26
```
Example 7: Fibonacci numbers recursive(Cont.)
```
```
B(n) = F (n + 1)
```
```
Hint:
```
```
where œÜ = (1 + ‚àö5)/2 ‚âà 1.61803 and œÜÀÜ = ‚àí1/œÜ ‚âà ‚àí0.61803
```
```
o Hence, A(n) ‚àà Œò(œÜn),
```
27
```
Example 8: Fibonacci numbers (Iterative)
```
o We can obtain a much
faster algorithm by
simply computing the
successive elements of
the Fibonacci sequence
iteratively.
o This algorithm clearly
makes n ‚àí 1 additions.
Hence, it is linear as a
```
function of n.
```
```
o Time complexity Œò(n)
```
Summary
o Recursive algorithm analysis
o Backward Substitution
o Recursive-Iterative comparison
28
Supportive Materials
```
o Introduction to The Design and Analysis of Algorithms (3rd Edition) by Anany
```
Levitin, Pearson.
o Introduction to Algorithms, by T.Cormen, C. Leiserson, R.Rivest and C.Stein,
MIT Press, 3rd Edition.
29
CSC 4520/6520
Design & Analysis of Algorithms
CHAPTER 3: BRUTE FORCE ALGORITHMS
1
Abdullah Bal, PhD
Objectives
Recap
- Backward Substitution
```
T(n) = T(n ‚àí 1) + n
```
```
T(n) = n*T(n ‚àí 1)
```
```
T(n) = T(n /3) + 1
```
```
T(n) = 2T(n ‚àí 1) + 1
```
- Tower of Honei
- Factorial
- Fibonacci Algorithms
Brute Force Selection Sort
Bubble Sort
String Matching
Closest pair problem
3
Brute Force
oBrute Force Algorithms are exactly what they sound like ‚Äì straightforward methods of solving a
problem that rely on sheer computing power and trying every possibility rather than advanced
techniques to improve efficiency.
o‚ÄúJust do it!‚Äù
oFor example, imagine you have a small padlock with 4 digits, each from 0-9. You forgot your
combination. How can you find out the correct combination?
```
oExamples:
```
```
o Computing an (a > 0, n a nonnegative integer)
```
oComputing n!
o Multiplying two matrices
oSearching for a key of a given value in a list
4
Brute Force
oThe brute-force approach should not be overlooked as an important algorithm design strategy.
oFirst, unlike some of the other strategies, brute force is applicable to a very wide variety of
problems.
oSecond, for some important problems‚Äîe.g., sorting, searching, matrix multiplication, string
matching‚Äî the brute-force approach yields reasonable algorithms of at least some practical value
with no limitation on instance size.
oThird, the expense of designing a more efficient algorithm may be unjustifiable if only a few
instances of a problem need to be solved and a brute-force algorithm can solve those instances
with acceptable speed.
oFourth, even if too inefficient in general, a brute-force algorithm can still be useful for solving
small-size instances of a problem.
oFinally, a brute-force algorithm can serve an important theoretical or educational purpose as a
yardstick with which to judge more efficient alternatives for solving a problem.
5
Selection Sort
```
https://codepumpkin.com/selection-sort-algorithms/6
```
Example 1: Selection Sort Algorithm
oScan the array to find its smallest element and swap it with the first element.
oThen, starting with the second element, scan the elements to the right of it to find the smallest
among them and swap it with the second elements.
```
oGenerally, on pass i (0 ¬£ i ¬£ n-2), find the smallest element in A[i..n-1] and swap it with A[i]:
```
A[0] ¬£ . . . ¬£ A[i-1] | A[i], . . . , A[min], . . ., A[n-1]
oAfter n ‚àí 1 passes, the list is sorted.
7
Analysis of Selection Sort
8
Analysis of Selection Sort
o The input size is given by the number of
```
elements n;
```
o The basic operation is the key comparison
A[j ] < A[min].
o The number of times it is executed
depends only on the array size.
```
o Thus, selection sort is a Œò(n2) algorithm on all inputs.
```
9
Example of Sorting with Selection Sort
o Each line corresponds to one iteration of the algorithm, i.e., a pass through the list‚Äôs tail to the right
```
of the vertical bar;
```
o An element in bold indicates the smallest element found.
o Elements to the left of the vertical bar are in their final positions and are not considered in this and
subsequent iterations.
o The action of the algorithm on the list 89, 45, 68, 90, 29, 34, 17 is illustrated as follow:
10
Bubble Sort Algorithm
11
Example 2: Bubble Sort Algorithm
oAnother brute-force application to the sorting problem is to compare adjacent
elements of the list and exchange them if they are out of order.
oBy doing it repeatedly, we end up ‚Äúbubbling up‚Äù the largest element to the last
position on the list.
oThe next pass bubbles up the second largest element, and so on, until after n ‚àí 1
```
passes the list is sorted. Pass i (0 ‚â§ i ‚â§ n ‚àí 2) of bubble sort can be represented by
```
the following diagram:
12
Analysis of Bubble Sort Algorithm
13
Analysis of Bubble Sort Algorithm
oThe number of key comparisons for the bubble-sort is the same for
```
all arrays of size n;
```
oIt is obtained by a sum that is almost identical to the sum for
selection sort:
14
Analysis of Bubble Sort Algorithm
oThe action of the algorithm on the list 89,
45, 68, 90, 29, 34, 17 is illustrated follow:
oFirst two passes of bubble sort on the list
89, 45, 68, 90, 29, 34, 17.
oA new line is shown after a swap of two
elements is done.
oThe elements to the right of the vertical
bar are in their final positions and are not
considered in subsequent iterations of the
algorithm.
15
Example 3: Brute-Force String Matching
```
opattern: a string of m characters to search for
```
```
otext: a (longer) string of n characters to search in (m ‚â§ n)
```
```
oproblem: find a substring in the text that matches the pattern
```
oWe want to find i‚Äîthe index of the leftmost character of the first matching substring in the text‚Äî
such that
```
ti =p0 ti+j = pj ti+m‚àí1=pm‚àí1
```
o If matches other than the first one need to be found, a string-matching algorithm can simply
continue working until the entire text is exhausted.
16
Brute-Force String Matching
oFor the string-matching problem:
o Align the pattern against the first m characters of the text and start matching
the corresponding pairs of characters from left to right until either all the m
```
pairs of the characters match (then the algorithm can stop) or a mismatching
```
pair is encountered.
o In the latter case, shift the pattern one position to the right and resume the
character comparisons, starting again with the first character of the pattern and
its counterpart in the text.
o The last position in the text that can still be a beginning of a matching substring
```
is n ‚àí m (provided the text positions are indexed from 0 to (n ‚àí 1).
```
o Beyond that position, there are not enough characters to match the entire
```
pattern; hence, the algorithm need not make any comparisons there.
```
17
Examples of Brute-Force String Matching
```
Pattern: 001011
```
```
Text: 10010101101001100101111010
```
```
Pattern: happy
```
```
Text: It is never too late to have a happy childhood.
```
18
Pseudocode and Efficiency
19
Pseudocode and Efficiency
o The pattern‚Äôs characters that are compared with their text counterparts are in
bold type.
```
o Searching in random texts, it has been shown to be linear, i.e., Œò(n).
```
20
Example 4: Closest-Pair Problem
21
Example 4: Closest-Pair Problem
22
Example 4: Closest-Pair Problem
23
Example 4: Closest-Pair Problem
```
oFind the two closest points in a set of n points (in the two-dimensional Cartesian plane).
```
oThese problems, arise in two important applied areas: computational geometry and operations
research.
oIt is the simplest of a variety of problems in computational geometry that deals with proximity of
points in the plane or higher-dimensional spaces.
oPoints in question can represent such physical objects as airplanes or post offices as well as
database records, statistical samples, DNA sequences, and so on.
oAn air-traffic controller might be interested in two closest planes as the most probable collision
candidates.
oA regional postal service manager might need a solution to the closest- pair problem to find
candidate post-office locations to be closed.
24
```
Example 4: Closest-Pair Problem(Cont.)
```
oOne of the important applications of the closest-pair problem is cluster analysis
in statistics.
oBased on n data points, hierarchical cluster analysis seeks to organize them in a
hierarchy of clusters based on some similarity metric.
```
oFor numerical data, this metric is usually the Euclidean distance;
```
oFor text and other nonnumerical data, metrics such as the Hamming distance
are used.
oA bottom-up algorithm begins with each element as a separate cluster and
merges them into successively larger clusters by combining the closest pair of
clusters.
25
Euclidian Distance
```
o Distance between two points pi(xi, yi ) and pj (xj , yj ) :
```
26
```
Hamming Distance (Binary)
```
Hamming distance = 3
27
```
Example 4: Closest-Pair Problem(Cont.)
```
o For simplicity, we consider the two-dimensional case of the closest-pair problem.
```
o We assume that the points in question are specified in a standard fashion by their (x, y)
```
```
Cartesian coordinates and that the distance between two points pi(xi, yi ) and pj (xj , yj ) is the
```
standard Euclidean distance
o The brute-force approach to solving this problem leads to the following obvious algorithm:
o Compute the distance between each pair of distinct points and find a pair with the smallest
distance.
o We do not want to compute the distance between the same pair of points twice. To avoid
```
doing so, we consider only the pairs of points (pi, pj) for which i < j.
```
28
```
Closest-Pair Brute-Force Algorithm (cont.)
```
29
```
Closest-Pair Brute-Force Algorithm (cont.)
```
o The basic operation of the algorithm will be squaring a number.
o The number of times it will be executed can be computed as follows:
====
30
Brute-Force Strengths and Weaknesses
oStrengths
oWide applicability
oSimplicity
oYields reasonable algorithms for some important problems
```
(e.g., matrix multiplication, sorting, searching, string matching)
```
31
Brute-Force Strengths and Weaknesses
oWeaknesses
oRarely yields efficient algorithms
oSome brute-force algorithms are unacceptably slow
oNot as constructive as some other design techniques
Summary
o Brute Force Algorithms
o Selection Sort
o Bubble Sort
o String Matching
o Closest pair problem
32
Supportive Materials
```
o Introduction to The Design and Analysis of Algorithms (3rd Edition) by Anany
```
Levitin, Pearson.
o Introduction to Algorithms, by T.Cormen, C. Leiserson, R.Rivest and C.Stein,
MIT Press, 3rd Edition.
33
CSC 4520/6520
Design & Analysis of Algorithms
CHAPTER 3: EXHAUSTIVE SEARCH ALGORITHMS
1
Abdullah Bal, PhD
Objectives
Recap
Brute Force
Selection Sort
Bubble Sort
String Matching
Closest pair problem
Exhaustive
Search
Travelling
Salesman
Problem
Knapsack Problem
Assignment problem
3
Exhaustive Search
oExhaustive search is simply a brute-force approach to combinatorial
problems.
oMany important problems require finding an element with a special
```
property in a domain that grows exponentially (or faster) with an
```
instance size.
oTypically, such problems arise in situations that involve‚Äîexplicitly or
implicitly‚Äîcombinatorial objects such as permutations,
combinations, and subsets of a given set.
4
Exhaustive Search
oMany such problems are optimization problems: they ask to find an
element that maximizes or minimizes some desired characteristic
such as a path length or an assignment cost.
oIt suggests generating each and every element of the problem
domain, selecting those of them that satisfy all the constraints, and
```
then finding a desired element (e.g., the one that optimizes some
```
```
objective function).
```
oAlthough the idea of exhaustive search is quite straightforward, its
implementation typically requires an algorithm for generating certain
combinatorial objects.
5
Exhaustive Search
oWe illustrate exhaustive search by applying it to three important
```
problems:
```
oThe traveling salesman problem
oThe knapsack problem
oThe assignment problem.
6
Exhaustive Search
```
Method:
```
‚ó¶ Generate a list of all potential solutions to the problem in a systematic manner.
‚ó¶ Evaluate potential solutions one by one, disqualifying infeasible ones and, for
an optimization problem, keeping track of the best one found so far
```
‚ó¶ When search ends, present the solution(s) found
```
7
```
Traveling Salesman Problem (TSP)
```
8
```
Example 1: Traveling Salesman Problem (TSP)
```
```
oThe traveling salesman problem (TSP) has been intriguing researchers for the
```
last 150 years by its seemingly simple formulation, important applications, and
interesting connections to other combinatorial problems.
oThe problem asks to find the shortest tour through a given set of n cities that
visits each city exactly once before returning to the city where it started.
oThe problem can be conveniently modeled by a weighted graph, with the
graph‚Äôs vertices representing the cities and the edge weights specifying the
distances.
9
```
Example 1: Traveling Salesman Problem (TSP)
```
oThe problem can be stated as the problem of finding the shortest
Hamiltonian circuit of the graph.
oA Hamiltonian circuit is defined as a cycle that passes through all the
vertices of the graph exactly once.
oIt is named after the Irish mathematician Sir William Rowan Hamilton
```
(1805‚Äì1865), who became interested in such cycles as an application
```
of his algebraic discoveries.
10
```
Example 1: Traveling Salesman Problem (TSP)
```
oA Hamiltonian circuit can also be defined as a sequence of
n + 1 adjacent vertices.
vi0 , vi1, . . . , v in‚àí1, vi0
where the first vertex of the sequence is the same as the
last one and all the other n ‚àí 1 vertices are distinct.
oAll circuits start and end at one particular vertex.
oWe can get all the tours by generating all the permutations
of n ‚àí 1 intermediate cities, compute the tour lengths, and
find the shortest among them.
a b
c d
8
2
7
5 3
4
11
```
Example 1: Traveling Salesman Problem (TSP)
```
oAn inspection of the figure reveals three pairs of tours that differ only by their
direction.
oHence, we could cut the number of vertex permutations by half.
oWe could, for example, choose any two intermediate vertices, say, b and c, and
then consider only permutations in which b precedes c.
```
oThe total number of permutations needed is still ¬Ω . (n ‚àí 1)!, which makes the
```
exhaustive-search approach impractical for all but very small values of n.
```
oTime complexity of the traveling salesman problem is Œò(n!).
```
oWe are not limited our investigation to the circuits starting at the same vertex.
o The number of permutations would have been even larger, by a factor of n.
a b
c d
8
2
7
5 3
4
12
Example 1: TSP by Exhaustive Search
Tour Cost
a‚Üíb‚Üíc‚Üíd‚Üía 2+3+7+5 = 17
a‚Üíb‚Üíd‚Üíc‚Üía 2+4+7+8 = 21
a‚Üíc‚Üíb‚Üíd‚Üía 8+3+4+5 = 20
a‚Üíc‚Üíd‚Üíb‚Üía 8+7+4+2 = 21
a‚Üíd‚Üíb‚Üíc‚Üía 5+4+3+8 = 20
a‚Üíd‚Üíc‚Üíb‚Üía 5+7+3+2 = 17
a b
c d
8
2
7
5 3
4
13
Knapsack Problem
14
Example 2: Knapsack Problem
o Given n items of known
weights w1,w2,...,wn
values v1,v2,...,vn
a knapsack of capacity W,
find the most valuable subset of the items that fit into the knapsack.
oThink about a transport plane that has to deliver the most valuable set of items to
a remote location without exceeding the plane‚Äôs capacity.
15
Example 2: Knapsack Problem
16
Example 2: Knapsack Problem
oThe exhaustive-search approach to this problem leads to
generating all the subsets of the set of n items given, computing the
```
total weight of each subset in order to identify feasible subsets (i.e.,
```
the ones with the total weight not exceeding the knapsack
```
capacity),
```
oFinding a subset of the largest value among them.
oSince the number of subsets of an n-element set is 2n, the
```
exhaustive search leads to a Œ©(2n) algorithm, no matter how
```
efficiently individual subsets are generated.
17
Evaluation of the Exhaustive Search
oFor both the traveling salesman and knapsack problems, exhaustive search
leads to algorithms that are extremely inefficient on every input.
oIn fact, these two problems are the best-known examples of No polynomial-
time algorithm is known for NP- hard problem
oMore-sophisticated approaches‚Äîbacktracking and branch-and-bound enable
us to solve some but not all instances of these and similar problems in less than
exponential time.
18
The Assignment Problem
19
Example 3: The Assignment Problem
oThere are n people who need to be assigned to n jobs, one person per job. The cost of assigning
person i to job j is C[i,j].
oFind an assignment that minimizes the total cost.
Job 0 Job 1 Job 2 Job 3
Person 0 9 2 7 8
Person 1 6 4 3 7
Person 2 5 8 1 8
Person 3 7 6 9 4
oAlgorithmic Plan: Generate all legitimate assignments, compute their costs and select the cheapest
one.
oHow many assignments are there?
20
Example 3: The Assignment Problem
oSince the number of permutations to be considered for the general case of the assignment problem is
n!, exhaustive search is impractical for all but very small instances of the problem.
o Pose the problem as the one about a cost matrix:
```
(first person, first job)
```
21
Example 3: The Assignment Problem
oWe can describe feasible solutions to the assignment problem as n-tuples
‚ü®j1, . . . , jn‚ü©
in which the ith component, i = 1, . . . , n, indicates the column of the element selected in the ith row
```
(i.e., the job number assigned to the ith person).
```
oFor example, for the cost matrix above, ‚ü®2, 3, 4, 1‚ü© indicates the assignment of
Person 1 to Job 2,
Person 2 to Job 3,
Person 3 to Job 4,
Person 4 to Job 1.
22
Example 3: The Assignment Problem
oThe requirements of the assignment problem imply that there is a one-to-one
correspondence between feasible assignments and permutations of the first n
integers.
oTherefore, the exhaustive-search approach to the assignment problem would
require generating all the permutations of integers 1, 2, . . . , n, computing the total
cost of each assignment by summing up the corresponding elements of the cost
matrix, and finally selecting the one with the smallest sum.
23
Example 3: The Assignment Problem
oIt is easy to see that an instance of the assignment problem is completely specified
by its cost matrix C.
oIn terms of this matrix, the problem is to select one element in each row of the
matrix so that all selected elements are in different columns and the total sum of the
selected elements is the smallest possible.
oNote that no obvious strategy for finding a solution works here.
oFor example, we cannot select the smallest element in each row, because the
smallest elements may happen to be in the same column.
oIn fact, the smallest element in the entire matrix need not be a component of an
optimal solution.
24
Final Comments on Exhaustive Search
oExhaustive-search algorithms run in a realistic amount of time only on very small
instances
oIn some cases, there are much better alternatives!
oEuler circuits
oShortest paths
oMinimum spanning tree
oIn many cases, exhaustive search or its variation is the only known way to get exact
solution
Summary
o Exhaustive Search
o Travelling Salesman Problem
o Knapsack Problem
o Assignment problem
25
Supportive Materials
```
o Introduction to The Design and Analysis of Algorithms (3rd Edition) by Anany
```
Levitin, Pearson.
o Introduction to Algorithms, by T.Cormen, C. Leiserson, R.Rivest and C.Stein,
MIT Press, 3rd Edition.
26
CSC 4520/6520
Design & Analysis of Algorithms
CHAPTER 3: GRAPH TRAVERSAL ALGORITHMS DFS-BFS
1
Abdullah Bal, PhD
Objectives
Recap
Exhaustive search
Travelling salesman problem
Knapsack problem
Assignment problem
Graph
Traversal
Depth First
```
Search (DFS)
```
Breadth First
```
Search (BFS)
```
23
Graph Traversal Algorithms
```
oMany problems require processing all graph vertices (and edges) in
```
systematic fashion
oGraph traversal algorithms:
```
oDepth-first search (DFS)
```
```
oBreadth-first search (BFS)
```
4
Graph Representation
```
https://www.geeksforgeeks.org/graph-and-its-representations/5
```
Tree
```
https://towardsdatascience.com/graph-theory-introduction-to-trees-a34ab267fc28
```
o Tree is an undirected graph with no cycles.
o All trees have N - 1 edges, where N is the number of nodes.
6
```
Depth-First Search (DFS)
```
```
https://brilliant.org/wiki/depth-first-search-dfs/
```
7
```
Depth-First Search (DFS)
```
oDepth-first search starts a graph‚Äôs traversal at an arbitrary vertex by marking it
as visited.
oOn each iteration, the algorithm proceeds to an unvisited vertex that is adjacent
to the one it is currently in.
oIf there are several such vertices, a tie can be resolved arbitrarily.
oAs a practical matter, which of the adjacent unvisited candidates is chosen is
dictated by the data structure representing the graph.
oIn our examples, we always break ties by the alphabetical order of the vertices.
8
```
Depth-First Search (DFS)
```
oThis process continues until a dead end‚Äîa vertex with no adjacent unvisited
vertices‚Äî is encountered.
oAt a dead end, the algorithm backs up one edge to the vertex it came from and
tries to continue visiting unvisited vertices from there.
oThe algorithm eventually halts after backing up to the starting vertex, with the
latter being a dead end.
oBy then, all the vertices in the same connected component as the starting
vertex have been visited.
oIf unvisited vertices still remain, the depth-first search must be restarted at any
one of them.
9
```
Depth-First Search (DFS)
```
o Visits graph‚Äôs vertices by always moving away from last visited vertex to unvisited one,
backtracks if no adjacent unvisited vertex is available.
oUses a stack
o a vertex is pushed onto the stack when it‚Äôs reached for the first time
o a vertex is popped off the stack when it becomes a dead end, i.e., when there is no
adjacent unvisited vertex
```
https://open4tech.com/stacks-vs-queues/10
```
```
Depth-First Search (DFS)
```
Stack
```
https://brilliant.org/wiki/depth-first-search-dfs/11
```
```
Example 1: Depth-First Search (DFS)
```
2
3
1
4
5
6
Stack
DFS Result:
o The essence of DFS: ‚ÄúGo into deep‚Äù
o Go as far from ‚Äúhome‚Äù as it can
12
```
Example 1: Depth-First Search (DFS)
```
2
3
1
4
5
6
Stack
DFS Result: 1
1
1
13
```
Example 1: Depth-First Search (DFS)
```
2
3
1
4
5
6
Stack
DFS Result: 1 2
2
1
1
2
14
```
Example 1: Depth-First Search (DFS)
```
2
3
1
4
5
6
Stack
DFS Result: 1 2 3
3
2
1
1
2
3
15
```
Example 1: Depth-First Search (DFS)
```
2
3
1
4
5
6
Stack
DFS Result: 1 2 3
2
1
1
2
3
16
```
Example 1: Depth-First Search (DFS)
```
2
3
1
4
5
6
Stack
DFS Result: 1 2 3 5
5
2
1
1
2
3 5
17
```
Example 1: Depth-First Search (DFS)
```
2
3
1
4
5
6
Stack
DFS Result: 1 2 3 5 4
4
5
2
1
1
2
3 5
4
18
```
Example 1: Depth-First Search (DFS)
```
2
3
1
4
5
6
Stack
DFS Result: 1 2 3 5 4
5
2
1
1
2
3 5
4
19
```
Example 1: Depth-First Search (DFS)
```
2
3
1
4
5
6
Stack
DFS Result: 1 2 3 5 4 6
6
5
2
1
1
2
3 5
4 6
20
```
Example 1: Depth-First Search (DFS)
```
2
3
1
4
5
6
Stack
DFS Result: 1 2 3 5 4 6
5
2
1
1
2
3 5
4 6
21
```
Example 1: Depth-First Search (DFS)
```
2
3
1
4
5
6
Stack
DFS Result: 1 2 3 5 4 6
2
1
1
2
3 5
4 6
22
```
Example 1: Depth-First Search (DFS)
```
2
3
1
4
5
6
Stack
DFS Result: 1 2 3 5 4 6
1
1
2
3 5
4 6
23
```
Example 1: Depth-First Search (DFS)
```
2
3
1
4
5
6
Stack
DFS Result: 1 2 3 5 4 6
1
2
3 5
4 6
24
Pseudocode of
DFS
25
```
Depth-First Search (DFS)
```
oThe brevity of the DFS pseudocode and the ease with which it can be performed
by hand may create a wrong impression about the level of sophistication of this
algorithm.
oHow efficient is depth-first search?
oIt is not difficult to see that this algorithm is, in fact, quite efficient since it takes
just the time proportional to the size of the data structure used for representing
the graph in question.
```
oThus, for the adjacency matrix representation, the traversal time is in Œò (|V|2),
```
```
oFor the adjacency list representation, it is in Œò (|V| + |E|).
```
26
```
Depth-First Search (DFS)
```
oA DFS forest is obtained as a by-product of a DFS traversal
oWe can look at it as the given graph with its edges classified by the DFS traversal
into two disjoint classes: tree edges and back edges.
oTree edges are edges used by the DFS traversal to reach previously unvisited
vertices.
oIf we consider only the edges in this class, we will indeed get a forest.
oBack edges connect vertices to previously visited vertices other than their
immediate predecessors in the traversal.
27
```
Depth-First Search (DFS)
```
oA DFS traversal itself and the forest-like representation of the graph it provides
have proved to be extremely helpful for the development of efficient algorithms
for checking many important properties of graphs.
oThe DFS yields two orderings of vertices: the order in which the vertices are
```
reached for the first time (pushed onto the stack) and the order in which the
```
```
vertices become dead ends (popped off the stack).
```
oThese orders are qualitatively different, and various applications can take
advantage of either of them.
28
```
Depth-First Search (DFS)
```
oImportant elementary applications of DFS include checking connectivity and
checking acyclicity of a graph.
oSince DFS halts after visiting all the vertices connected by a path to the starting
vertex, checking a graph‚Äôs connectivity can be done as follows.
oStart a DFS traversal at an arbitrary vertex and check, after the algorithm halts,
whether all the vertices of the graph will have been visited.
```
oIf they have, the graph is connected; otherwise, it is not connected.
```
oMore generally, we can use DFS for identifying connected components of a graph.
29
```
Depth-First Search (DFS)
```
oAs for checking for a cycle presence in a graph, we can take advantage of the graph‚Äôs
representation in the form of a DFS forest.
oIf the latter does not have back edges, the graph is clearly acyclic.
```
oIf there is a back edge from some vertex u to its ancestor v (e.g., the back edge from d to a), the
```
graph has a cycle that comprises the path from v to u via a sequence of tree edges in the DFS
forest followed by the back edge from u to v.
30
```
Example 2: Depth-First Search (DFS)
```
oStarting at vertex ‚Äúa‚Äù and resolving ties by the vertex alphabetical order, traverse the graph by
depth-first search and write depth-first search list and illustrate DFS tree.
31
```
Example 2: Depth-First Search (DFS)
```
Graph
```
Traversal‚Äôs stack (the first subscript number
```
indicates the order in which a vertex is
```
visited, i.e., pushed onto the stack; the
```
second one indicates the order in which it
becomes a dead-end, i.e., popped off the
```
stack).
```
DFS forest with the tree
and back edges shown
with solid and dashed
lines, respectively.
32
```
Example 3: Depth-First Search (DFS)
```
oStarting at vertex ‚Äú0‚Äù and resolving ties by the vertex alphabetical order, traverse the graph by
depth-first search and write down depth-first search list.
33
```
Example 3: Depth-First Search (DFS)
```
```
oSolution:
```
DFS Result: 0 1 2 3 4
0
1
3
2
4
34
Main Points of DFS
oDFS can be implemented with graphs represented as:
```
o adjacency matrices: Œò(|V|2)
```
```
o adjacency lists: Œò(|V|+|E|)
```
oYields two distinct ordering of vertices:
```
o Order in which vertices are first encountered (pushed onto stack)
```
```
o Order in which vertices become dead-ends (popped off stack)
```
```
oApplications:
```
o Checking connectivity, finding connected components
o Checking acyclicity
o Finding articulation points and biconnected components
```
o Searching state-space of problems for solution (AI)
```
35
```
Breadth-first search (BFS)
```
36
```
Breadth-first search (BFS)
```
oBreadth-first search is a traversal for the cautious.
oIt proceeds in a concentric manner by visiting first all the vertices that
are adjacent to a starting vertex, then all unvisited vertices two edges
apart from it, and so on, until all the vertices in the same connected
component as the starting vertex are visited.
oIf there still remain unvisited vertices, the algorithm has to be
restarted at an arbitrary vertex of another connected component of
the graph.
37
```
Breadth-first search (BFS)
```
oIt is convenient to use a queue to trace the operation of breadth-first search.
oThe queue is initialized with the traversal‚Äôs starting vertex, which is marked as visited.
oOn each iteration, the algorithm identifies all unvisited vertices that are adjacent to the front vertex,
```
marks them as visited, and adds them to the queue; after that, the front vertex is removed from the
```
queue.
oVisits graph vertices by moving across to all the neighbors of last visited vertex
38
```
Example 1: Breadth-first search (BFS)
```
2
3
1
4
5
6
Queue
BFS Result:
o The essence of BFS: ‚ÄúExplore around‚Äù
39
```
Example 1: Breadth-first search (BFS)
```
2
3
1
4
5
6
Queue
BFS Result: 1
1
40
```
Example 1: Breadth-first search (BFS)
```
2
3
1
4
5
6
Queue
BFS Result: 1 2
2 1
1
2
41
```
Example 1: Breadth-first search (BFS)
```
2
3
1
4
5
6
Queue
BFS Result: 1 2 3
3 2 1
1
2 3
42
```
Example 1: Breadth-first search (BFS)
```
2
3
1
4
5
6
Queue
BFS Result: 1 2 3 6
6 3 2 1
1
2 3 6
43
```
Example 1: Breadth-first search (BFS)
```
2
3
1
4
5
6
Queue
BFS Result: 1 2 3 6
6 3 2
1
2 3 6
44
```
Example 1: Breadth-first search (BFS)
```
2
3
1
4
5
6
Queue
BFS Result: 1 2 3 6 5
5 6 3 2
1
2 3 6
5
45
```
Example 1: Breadth-first search (BFS)
```
2
3
1
4
5
6
Queue
BFS Result: 1 2 3 6 5
5 6 3
1
2 3 6
5
46
```
Example 1: Breadth-first search (BFS)
```
2
3
1
4
5
6
Queue
BFS Result: 1 2 3 6 5
5 6
1
2 3 6
5
47
```
Example 1: Breadth-first search (BFS)
```
2
3
1
4
5
6
Queue
BFS Result: 1 2 3 6 5
5
1
2 3 6
5
48
```
Example 1: Breadth-first search (BFS)
```
2
3
1
4
5
6
Queue
BFS Result: 1 2 3 6 5 4
4 5
1
2 3 6
5
4
49
```
Example 1: Breadth-first search (BFS)
```
2
3
1
4
5
6
Queue
BFS Result: 1 2 3 6 5 4
4
1
2 3 6
5
4
50
```
Example 1: Breadth-first search (BFS)
```
2
3
1
4
5
6
Queue
BFS Result: 1 2 3 6 5 4
1
2 3 6
5
4
51
DFS-BFS
2
3
1
4
5
6
BFS Result: 1 2 3 6 5 4
DFS Result: 1 2 3 5 4 6
52
Pseudocode of BFS
53
Example 2: BFS Traversal
oStarting at vertex ‚Äúa‚Äù and resolving ties by the vertex alphabetical order, traverse the graph by
breadth-first search and write breadth-first search list and illustrate BFS tree.
54
Example 2: BFS Traversal
Graph
Traversal queue, with the numbers
indicating the order in which the vertices
```
are visited, i.e., added to (and removed
```
```
from) the queue.
```
BFS forest with the tree
and cross edges shown
with solid and dotted lines
55
```
Example 3: Breadth-first search (BFS)
```
oStarting at vertex ‚Äú0‚Äù and resolving ties by the vertex alphabetical order, traverse the graph by
breadth-first search and write down breadth-first search list.
56
```
Example 3: Breadth-first search (BFS)
```
```
oSolution:
```
BFS Result: 0 1 2 3 4
0
1 32
4
57
BFS Traversal
oIllustration of the BFS-based algorithm for finding a minimum-edge
path.
Graph
Part of its BFS tree that identifies the minimum-
edge path from a to g
58
```
Breadth-first search (BFS)
```
o Breadth-first search has the same efficiency as depth-first search.
```
o It is in Œò(|V |2) for the adjacency matrix representation and in Œò(|V | + |E|)
```
for the adjacency list representation.
o Unlike depth-first search, it yields a single ordering of vertices because the
```
queue is a FIFO (first-in first-out) structure and hence the order in which
```
vertices are added to the queue is the same order in which they are
removed from it.
59
```
Breadth-first search (BFS)
```
o As to the structure of a BFS forest of an undirected graph, it can also have
two kinds of edges: tree edges and cross edges.
o Tree edges are the ones used to reach previously unvisited vertices.
o Cross edges connect vertices to those visited before
60
```
Breadth-first search (BFS)
```
o BFS can be used to check connectivity and acyclicity of a graph, essentially in
the same manner as DFS can.
o It is not applicable, however, for several less straightforward applications such
as finding articulation points.
o On the other hand, it can be helpful in some situations where DFS cannot.
61
```
Breadth-first search (BFS)
```
o For example, BFS can be used for finding a path with the fewest
number of edges between two given vertices.
o To do this, we start a BFS traversal at one of the two vertices and
stop it as soon as the other vertex is reached.
o The simple path from the root of the BFS tree to the second
vertex is the path sought.
o For example, path a ‚àí b ‚àí c ‚àí g in the graph has the fewest
number of edges among all the paths between vertices a and g.
62
Main Points of BFS
oBFS has same efficiency as DFS and can be implemented with graphs
represented as:
```
oAdjacency matrices: Œò(|V|2)
```
```
oAdjacency lists: Œò(|V|+|E|)
```
```
oYields single ordering of vertices (order added/deleted from queue is the same)
```
```
oApplications: Same as DFS, but can also find paths from a vertex to all other
```
vertices with the smallest number of edges
63
DFS vs. BFS
```
o Main facts about depth-first search (DFS) and breadth-first search (BFS)
```
Summary
o Graph Traversal
```
o Depth First Search (DFS)
```
```
o Breadth First Search (BFS)
```
64
Supportive Materials
```
o Introduction to The Design and Analysis of Algorithms (3rd Edition) by Anany
```
Levitin, Pearson.
o Introduction to Algorithms, by T.Cormen, C. Leiserson, R.Rivest and C.Stein,
MIT Press, 3rd Edition.
65